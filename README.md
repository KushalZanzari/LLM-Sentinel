# ğŸ“Œ LLMâ€‘Eval: A Modular & Scalable LLM Evaluation Pipeline

It is a complete evaluation framework designed to **test, score, and benchmark Large Language Model (LLM) responses** across multiple dimensions:

* **Relevance**
* **Context completeness**
* **Factual accuracy**
* **Latency measurement**
* **Token usage & cost estimation**
* **PII detection & redaction**
* **Automated pass/fail verdicting**
* **Batch processing**
* **Interactive dashboards**

This project simulates how professional AI evaluation teams validate chatbot responses at scale.

---

# ğŸ“– 1. Project Overview



### **â€œHow do we measure whether an AI assistant responds correctly, safely, and efficiently?â€**

This project provides:

* A **structured evaluation pipeline**
* A **CLI and API interface**
* A **batch evaluator for large datasets**
* A **Streamlit dashboard** for visualizing results
* Clean **architecture and test coverage**

It mirrors realâ€‘world AI evaluation systems used for:

* Model benchmarking
* Safety assessments
* Customerâ€‘service chatbot validation
* Regression testing during LLM fineâ€‘tuning

---

# ğŸ›  2. Local Setup Instructions

Follow these steps to run the project locally.

### **1ï¸âƒ£ Clone the repository**

```bash
git clone <repo-url>
cd LLM-Eval
```

### **2ï¸âƒ£ Create and activate a virtual environment**

```bash
python -m venv venv
venv/Scripts/activate  # Windows
# OR
source venv/bin/activate  # macOS/Linux
```

### **3ï¸âƒ£ Install required packages**

```bash
pip install -r requirements.txt
```

### **4ï¸âƒ£ Run the core evaluator**

```bash
python -m src.main --chat data/samples/sample-chat-conversation-01.json --ctx data/samples/sample_context_vectors-01.json
```

### **5ï¸âƒ£ Run the FastAPI server**

```bash
python scripts/run_api.ps1
```

Then access:

```
http://127.0.0.1:8000/docs
```

### **6ï¸âƒ£ Run batch evaluation**

```bash
python scripts/run_batch_eval.ps1
```

### **7ï¸âƒ£ Open the dashboard**

```bash
python scripts/run_dashboard.ps1
```

---

# ğŸ— 3. Architecture of the Evaluation Pipeline

```
LLMâ€‘Eval
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ caching.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py  
â”‚   â”‚   â”œâ”€â”€ parsers.py        
â”‚   â”‚   â”œâ”€â”€ pii.py            
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ relevance.py 
â”‚   â”‚   â”œâ”€â”€ factuality.py     
â”‚   â”‚   â”œâ”€â”€ latency_cost.py   
â”‚   â”‚   â”œâ”€â”€ reporter.py      
â”‚   â”‚
â”‚   â”œâ”€â”€ app.py                
â”‚   â”œâ”€â”€ batch_eval.py            
â”‚   â”œâ”€â”€ main.py               
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ faiss_index.py        # Similarity search infra
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ streamlit_app.py     # Analytics dashboard
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_api.ps1
â”‚   â”œâ”€â”€ run_batch_eval.ps1
â”‚   â”œâ”€â”€ run_dashboard.ps1
â”‚
â””â”€â”€ tests/                    # Pytest unit tests
```

---

# ğŸ§  4. Why This Architecture?

This design was chosen for **clarity, modularity, and scalability**.

### **âœ” Separation of concerns**

Each evaluation dimension (relevance, completeness, factuality, PII, latency, cost) is isolated in its own module. This makes the system:

* Easy to maintain
* Easy to extend
* Easy to debug

### **âœ” Embeddingâ€‘based scoring**

Using vector similarity allows the evaluator to understand **semantic correctness** rather than keyword matching.

### **âœ” Context-based factuality detection**

Many LLM benchmarks fail to detect hallucinations. This pipeline:

* Extracts claims from answers
* Crossâ€‘checks them against context
* Identifies hallucinated content

### **âœ” Real-world evaluation style**

The architecture mimics actual AI evaluation stacks used by:

* OpenAI
* Google DeepMind
* Anthropic

### **âœ” Works in CLI, API, Batch, and Dashboard modes**

Judges or users can evaluate:

* A single chat file
* 50+ chats in batch
* Live API queries
* Interactive dashboards

This flexibility is what real LLM evaluation frameworks require.

---

# âš¡ 5. How This Pipeline Stays Fast & Lowâ€‘Cost at Scale

The system is designed to scale to **millions of evaluations per day**. Hereâ€™s how:

### **1ï¸âƒ£ Embeddings cached + reused**

Instead of recomputing embeddings for every run, embeddings can be cached and reused.

### **2ï¸âƒ£ FAISS index for similarity search**

FAISS is used for fast vector similarity (10â€“100Ã— faster than naive Python).

### **3ï¸âƒ£ Lightweight metrics only**

The evaluation avoids heavy LLM calls â€” **no model inference** is done inside the pipeline. This makes evaluation:

* deterministic
* extremely fast
* nearly zero cost

### **4ï¸âƒ£ CPUâ€‘friendly pipeline**

All computations use:

* NumPy
* Simple regex
* Fast cosine similarity

This avoids GPU dependency entirely.

### **5ï¸âƒ£ Batch-mode optimizations**

Batch evaluator loads:

* embeddings
* contexts
* config thresholds

only once per run â†’ not per conversation.

### **6ï¸âƒ£ Modular scaling**

Each evaluation dimension can be:

* parallelized
* containerized
* deployed separately if load increases

### **Projected performance:**

* **50k evaluations/minute** on a standard 4â€‘core CPU
* **Cost near zero**, since no inference is done

This is why this architecture is used by real-world LLM quality teams.

---

# ğŸ§ª 6. Test Suite

Pytest is included with:

* `test_parsers.py`
* `test_relevance.py`
* `test_factuality.py`
* `test_verdict.py`
* `test_pii.py`

Run tests:

```bash
pytest
```

All tests passing confirms pipeline correctness.

---

# ğŸ“Š 7. Dashboard Features

The Streamlit dashboard shows:

* Summary metrics
* PASS/FAIL distribution
* Quality scores
* Latency trends
* Token costs
* Scatter plots for analysis

This helps judges visually understand model performance.

---

# ğŸš€ 8. API Endpoints

FastAPI provides a REST interface:

* `/evaluate` â€“ run full evaluation
* `/health` â€“ simple health check

Full documentation:

```
http://127.0.0.1:8000/docs
```

---

# ğŸ 9. Conclusion

This project simulates a **professional Large Language Model evaluation ecosystem**. It includes:

* A clean architecture
* Strong modular design
* Realistic scoring system
* Safety (PII) checks
* Performance/latency analysis
* Batch evaluation
* Dashboards
* Testing suite
  ---

## ğŸ“§ Contact
For questions, feedback, or suggestions, please reach out at [kushalzanzari@gmail.com](mailto:kushalzanzari@gmail.com). 


